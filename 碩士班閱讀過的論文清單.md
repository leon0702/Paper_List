# 閱讀過的論文清單
**[1]** J. Turian, L. Ratinov and Y. Bengio, "Word representations: A simple and general method for semi-supervised learning," in Proc. ACL 2010.

**[2]** T. Mikolov, K. Chen, G. Corrado and J. Dean, "Efficient Estimation of Word Representations in Vector Space," in Proc. ICLR 2013.

**[3]** G. Andrew, R. Arona, J. Bilmes and K. Livescu, "Deep Canonical Correlation Analysis," in Proc ICML 2013.

**[4]** I. Sutskever, O. Vinyals, Q. Le, "Sequence to Sequence Learning with Neural Networks," in Proc. NIPS 2014.

**[5]** W. Wang, R Arora, K. Livescu, J. Bilmes, "On Deep Multi-View Representation Leaning," ICML 2015.

**[6]** M. Raruqui, J. Dodge, S. Jauhar, C. Dyer, E. Hovy and N. Smith, "Retrofitting Word Vectors to Semantic Lexicons," in Proc. NAACL 2015.

**[7]** D. Bollegala, A. Mohammed, T. Maehara and K. Kawarabayashi, "Joint Word Representation Learning using a Corpus and a Semantic Lexicon," in Proc. AAAI 2016.

**[8]** G. Ma, X. Yang, B. Zhang and Z. Shi, "Multi-geature Fusion Deep Networks," Neurocomputing 2016.

**[9]** T. Shen, T. Lei, R. Barzilay and T. Jaakkola, "Style Transfer from Non-parallel Text by Cross-Alignment," in Proc. NIPS 2017.

**[10]** A. Tarvainen and H. Valpola, "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results," in Proc. NIPS 2017.

**[11]** G. Brunner, Y. Wang, R. Wattenhofer and M. Weigelt, "Natural Language Multitasking," in Proc. NIPS 2017.

**[12]** T. Young, D. Hazarika, S. Poria and E. Cambria, "Recent Trends in Deep Learning Based Natural Language Processing," Computational Intelligence Magazine 2018.

**[13]** A. Utsumi, "Refining Pretrained Word Embeddings Using Layer-wise Relevance Propagation," in Proc. NIPS 2018.

**[14]** P. Bojanowski, E. Grave, A. Joulin and T. Mikolov, "Enriching Word Vectors with Subword Information," in Porc. TACL 2017.

**[15]** L. Yu, J. Wang, K. Lai and X. Zhang, "Refining Word Embeddings for Sentiment Analysis," in Proc. EMNLP 2017.

**[16]** B. Liu, T. Zgang, F. Han, D. Niu, K. Lai and Y. Xu, "Matching Natural Language Sentences with Hierarchical Sentence Factorization," IW3C2 2018.

**[17]** A. Conneau, D. Kiela, H. Schwend, L. Barrault and A. Bordes, "Supervised Learning of Universal Sentence Representations from Natural Language Inference Data," in Proc. EMNLP 2017.

**[18]** G. Patrini, A. Rozza, A. Menon, R. Nock and L. Qu, "Making Deep Neural Networks Robust to Label Noise: a Loss Correction Approach," in Proc. CVPR 2017.

**[19]** A. Radford, K. Narasimhan, T. Salimans and I. Sutskever, "Improving Language Understanding by Generative Pre-Training," . https://s3-us-west-2.amazonaws.com/openai-
assets/research-covers/language-unsupervised/language_understanding_paper.pdf, 2018.

**[20]** B. Han, Q. Yao, X. Yu, G. Niu, M. Xu, W. Hu, I. Tsang, and M. Sugiyama, “Co-teaching: Robust training of deep neural networks with extremely noisy labels,” in Proc. 
NIPS, pp. 8527-8537, 2018.

**[21]** F. Barigou, "Impact of Instance Selection on kNN-Based Text Categorization," in Proc. JIPS 2018.

**[22]** L. Yu, J. Wang, K. Lai and X. Zhang, "Refining Word Embeddings Using Intensity Scores for Sentiment Analysis," IEEE/ACM Transactions on Audio, Speech, and Language processing, 2018.

**[23]** Z. Fu, X, Tan, N. Peng, D. Zhao and R. Yan, "Style Transfer in Text: Exploration and Evaluation," in Proc. AAAI 2018.

**[24]** D. Kiela, C. Wang and K. Cho, "Dynamic Meta-Embeddings for Improved Sentence Representations," in Proc. EMNLP 2018.

**[25]** Q. Liu, H. Huang, Y. Gao, X. Wei, Y. Tian and L. Liu, "Task-oriented Word Embedding for Text Classification," in Proc. COLING 2018.

**[26]** H. Liu, J. Wang, S. Li, J. Li and G. Zhou, "Semi-supervised Sentiment Classification Based on Auxiliary Task Learning," in Proc. NLPCC 2018.

**[27]** T. Ager, O. Kuzelka, S. Schockaert, "Modelling Salient Features as Directions in Fine-Tuned Semantic Spaces," in Proc. CoNLL 2018.

**[28]** Z. Yang, Z. Hu, C. Dyer, E. Xing, T. Berg-Kirkpatrick, "Unsupervised Text Style Transfer using Language Models as Discriminators," in Proc. NIPS 2018.

**[29]** J. Devlin, M. Chang, K. Lee and K. Toutanova, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding," NAACL 2019.

**[30]** Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. Le and R. Salakhutdinov, "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context," in Proc. ACL 2019.

**[31]** Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. Salakhutdinov and Q. Le, "XLNet: Generalized Autoregressive Pretraining for Lnaguage Understanding," in Proc. NIPS 2019.

**[32]** X. Yu, B. Han, J. Yao, G. Niu, I. W. Tsang, and M. Sugiyama, “How does disagreement help generalization against label corruption?” in Proc. ICML, 2019.

**[33]** Q. Li, H. Peng, J. Li, C. Xia, R. Yang, L. Sun, P. Yu and L. He, "A Survey on Text Classification: From Shallow to Deep Learning," IEEE Transactions on neural networks and learning systems, 2020.

**[34]** J. Li, R. Socher and S. Hoi, "DivideMix: Learning with Noisy Labels as Semi-supervised Learning," in Proc. ICLR 2020.

**[35]** D. Nguyen, C. Mummadi, T. Ngo, T. Nguyen, L. Beggel and T. Brox, "SELF: Learning to Filter Noisy Labels with Self-Ensembling," ICLR 2020.

**[36]** H. Song, M. Kim, D. Park and J. Lee, "Learning from Noisy Labels with Deep Neural Networks: A Survey," arXiv:2007.08199v3 [cs.LG] 28 Oct 2020.

**[37]** Q. Xie, Z. Dai, E. Hovy, M. Luong and Q. Le, "Unsupervised Data Augmentation for Consistency Training," in Proc. NIPS 2020.

